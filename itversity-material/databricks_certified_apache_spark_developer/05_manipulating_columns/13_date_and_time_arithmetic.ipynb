{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00859e09-7385-4755-afc5-040f83fb9147",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Date and Time Arithmetic\n",
    "Let us perform Date and Time Arithmetic using relevant functions over Spark Data Frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05844583-1b64-4ab5-870c-5939f2742156",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting the number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed46b882-aad9-452d-bd5d-3239968dced1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform some tasks related to date arithmetic.\n",
    "* Get help on each and every function first and understand what all arguments need to be passed.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ce5c540-ba8c-4617-9d05-8552a11768a0",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfa5eb4c-b258-4c64-8e02-654a2e1bf9b3",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "954ca8ec-c653-4712-a2cc-d1785776ae78",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b67718d-6701-4baf-8558-4917d5576d52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Add 10 days to both date and time values.\n",
    "* Subtract 10 days from both date and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "928c4fe9-d43a-49d1-98c9-a9d09187f849",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae7fa9e1-aa80-4819-8bea-fe2d341030a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "703e487d-68ce-438d-9f88-ef07e1d72985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_sub in module pyspark.sql.functions:\n",
      "\n",
      "date_sub(start, days)\n",
      "    Returns the date that is `days` days before `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "    [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de3aa7ce-7553-4d16-92cc-c6016d99037e",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-03-10|   2014-02-18|   2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-03-10|   2016-02-19|   2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2018-01-10|   2017-10-21|   2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-09-10|   2019-11-20|   2019-08-21|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_add_date\", date_add(\"date\", 10)). \\\n",
    "    withColumn(\"date_add_time\", date_add(\"time\", 10)). \\\n",
    "    withColumn(\"date_sub_date\", date_sub(\"date\", 10)). \\\n",
    "    withColumn(\"date_sub_time\", date_sub(\"time\", 10)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "171e4075-8f88-4449-99f4-a80471b07be8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Get the difference between current_date and date values as well as current_timestamp and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92b8a45b-a2b1-4739-8989-f4158f449bd8",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60d23832-bd9e-4fa8-ab6f-15ad25becbf2",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+\n",
      "|      date|                time|datediff_date|datediff_time|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|         2920|         2920|\n",
      "|2016-02-29|2016-02-29 08:08:...|         2189|         2189|\n",
      "|2017-10-31|2017-12-31 11:59:...|         1579|         1518|\n",
      "|2019-11-30|2019-08-31 00:00:...|          819|          910|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"datediff_date\", datediff(current_date(), \"date\")). \\\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp(), \"time\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55b21f07-7e54-4eee-aa9e-7ce7e964c207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "* Add 3 months to both date values as well as time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d43730c-a779-4697-86b5-8c3821b401e4",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab851b7a-11b3-45d8-96a7-05d4e111aec6",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|95.94              |95.93              |2014-05-28     |2014-05-28     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|71.9               |71.9               |2016-05-29     |2016-05-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|51.84              |49.83              |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|26.87              |29.84              |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date(), \"date\"), 2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(), \"time\"), 2)). \\\n",
    "    withColumn(\"add_months_date\", add_months(\"date\", 3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\", 3)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd30000d-60ad-4b5d-8d67-ed23793f05fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 Date and Time Arithmetic",
   "notebookOrigID": 456917195748777,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3 (local)",
   "language": "python",
   "name": "pyspark3local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
