{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5430d9dc-601c-4959-905d-ec97b9be1c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col, lit, concat, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a136d179-553b-4317-93c5-e62c5d621970",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Whenever possible, use specialized functions like `year`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "233a1dcb-b8b6-4bec-bb23-a95c66343afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function col in module pyspark.sql.functions:\n",
      "\n",
      "col(col)\n",
      "    Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1051c02-c19e-40ab-b6ae-451007804666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lit in module pyspark.sql.functions:\n",
      "\n",
      "lit(col)\n",
      "    Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
      "    [Row(height=5, spark_user=True)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e9b5230-7eb7-469b-b1ca-394b32c3cd1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pyspark.sql.functions:\n",
      "\n",
      "concat(*cols)\n",
      "    Concatenates multiple input columns together into a single column.\n",
      "    The function works with strings, binary and compatible array columns.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "    [Row(s='abcd123')]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n",
      "    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "194a9422-a623-47c7-9c17-468c73f0ffb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions:\n",
      "\n",
      "concat_ws(sep, *cols)\n",
      "    Concatenates multiple input string columns together into a single string column,\n",
      "    using the given separator.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "    [Row(s='abcd-123')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(concat_ws)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05 Getting Help on Spark Functions",
   "notebookOrigID": 456917195748683,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3 (local)",
   "language": "python",
   "name": "pyspark3local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
