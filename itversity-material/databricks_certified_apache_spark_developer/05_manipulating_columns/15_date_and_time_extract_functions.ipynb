{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6748b0e2-6ced-472d-9808-7791974d3972",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Date and Time Extract Functions\n",
    "Let us get an overview about Date and Time extract functions. Here are the extract functions that are useful which are self explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5e89dac-50bc-41e8-996c-ea7e82b9da44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* `year`\n",
    "* `month`\n",
    "* `weekofyear`\n",
    "* `dayofyear`\n",
    "* `dayofmonth`\n",
    "* `dayofweek`\n",
    "* `hour`\n",
    "* `minute`\n",
    "* `second`\n",
    "\n",
    "There might be few more functions. You can review based up on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e4bfa6d-1bc4-4594-9719-ecf1034323a8",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l = [(\"X\", )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dfb2b51e-ccb0-45ef-aecd-ee2011f3375c",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fbad2822-d679-4516-9c1a-45cef06dd9ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2bd0f360-5434-4888-a802-048e1a0d14bc",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, weekofyear, dayofmonth, \\\n",
    "    dayofyear, dayofweek, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "296203c0-2415-40e6-94ce-2863af158a3d",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "|  2022-02-26|2022|    2|         8|       57|        26|        7|\n",
      "+------------+----+-----+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_date().alias('current_date'), \n",
    "    year(current_date()).alias('year'),\n",
    "    month(current_date()).alias('month'),\n",
    "    weekofyear(current_date()).alias('weekofyear'),\n",
    "    dayofyear(current_date()).alias('dayofyear'),\n",
    "    dayofmonth(current_date()).alias('dayofmonth'),\n",
    "    dayofweek(current_date()).alias('dayofweek')\n",
    ").show() #yyyy-MM-dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2d86e36-d0e2-446f-82bf-fb17ac53e738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dayofweek in module pyspark.sql.functions:\n",
      "\n",
      "dayofweek(col)\n",
      "    Extract the day of the week of a given date as integer.\n",
      "    \n",
      "    .. versionadded:: 2.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "    [Row(day=4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35ed882c-f24d-462a-aa23-5275f1b7a60d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94eb58e0-cb8e-48b6-a4c9-629830883bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc319e2b-f75f-49e6-b603-16670070ea1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "    \n",
      "    acosh(col)\n",
      "        Computes inverse hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "    \n",
      "    aggregate(col, initialValue, merge, finish=None)\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        >>> df.select(\n",
      "        ...     aggregate(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`approx_count_distinct` instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n",
      "        of column `col`.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        rsd : float, optional\n",
      "            maximum relative standard deviation allowed (default = 0.05).\n",
      "            For rsd < 0.01, it is more efficient to use :func:`countDistinct`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n",
      "        [Row(distinct_ages=2)]\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "            the same data type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            value or column to check for in array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    \n",
      "    array_distinct(col)\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "    \n",
      "    array_except(col1, col2)\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "    \n",
      "    array_intersect(col1, col2)\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "    \n",
      "    array_join(col, delimiter, null_replacement=None)\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "    \n",
      "    array_max(col)\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "    \n",
      "    array_min(col)\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "    \n",
      "    array_position(col, value)\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "        value could not be found in the array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "    \n",
      "    array_remove(col, element)\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        element :\n",
      "            element to be removed from the array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "    \n",
      "    array_repeat(col, count)\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "    \n",
      "    array_sort(col)\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    array_union(col1, col2)\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "    \n",
      "    arrays_overlap(a1, a2)\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "    \n",
      "    arrays_zip(*cols)\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns of arrays to be merged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
      "        >>> df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()\n",
      "        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    asc_nulls_first(col)\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    asc_nulls_last(col)\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "    \n",
      "    asinh(col)\n",
      "        Computes inverse hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    assert_true(col, errMsg=None)\n",
      "        Returns null if the input column is true; throws an exception with the provided error message\n",
      "        otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "    \n",
      "    atan(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on y-axis\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on x-axis\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the `theta` component of the point\n",
      "            (`r`, `theta`)\n",
      "            in polar coordinates that corresponds to the point\n",
      "            (`x`, `y`) in Cartesian coordinates,\n",
      "            as if computed by `java.lang.Math.atan2()`\n",
      "    \n",
      "    atanh(col)\n",
      "        Computes inverse hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "    \n",
      "    bucket(numBuckets, col)\n",
      "        Partition transform function: A transform for any type that partitions\n",
      "        by a hash of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     bucket(42, \"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_set('age')).collect()\n",
      "        [Row(collect_set(age)=[5, 2])]\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, binary and compatible array columns.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n",
      "        ``col1`` and ``col2``.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "    \n",
      "    cos(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "    \n",
      "    cosh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
      "        column. All calls of current_timestamp within the same query return the same value.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of `datetime pattern`_. can be used.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Whenever possible, use specialized functions like `year`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    date_trunc(format, timestamp)\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n",
      "            'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofweek(col)\n",
      "        Extract the day of the week of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "    \n",
      "    days(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into days.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     days(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    desc_nulls_first(col)\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    desc_nulls_last(col)\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    element_at(col, extraction)\n",
      "        Collection function: Returns element of array at given index in extraction if col is array.\n",
      "        Returns value for the given key in extraction if col is map.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a'), Row(element_at(data, 1)=None)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exists(col, f)\n",
      "        Returns whether a predicate holds for one or more elements in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        :return: a :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
      "        >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n",
      "        +------------+\n",
      "        |any_negative|\n",
      "        +------------+\n",
      "        |       false|\n",
      "        |        true|\n",
      "        +------------+\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "    \n",
      "    explode_outer(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|null| null|\n",
      "        |  3|      null|null| null|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|{x -> 1.0}| foo|\n",
      "        |  1|{x -> 1.0}| bar|\n",
      "        |  2|        {}|null|\n",
      "        |  3|      null|null|\n",
      "        +---+----------+----+\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "    \n",
      "    filter(col, f)\n",
      "        Returns an array of elements for which a predicate holds in a given array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            A function that returns the Boolean expression.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> def after_second_quarter(x):\n",
      "        ...     return month(to_date(x)) > 6\n",
      "        >>> df.select(\n",
      "        ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |after_second_quarter    |\n",
      "        +------------------------+\n",
      "        |[2018-09-20, 2019-07-01]|\n",
      "        +------------------------+\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "    \n",
      "    flatten(col)\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.select(flatten(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    forall(col, f)\n",
      "        Returns whether a predicate holds for every element in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n",
      "        +-------+\n",
      "        |all_foo|\n",
      "        +-------+\n",
      "        |  false|\n",
      "        |  false|\n",
      "        |   true|\n",
      "        +-------+\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            the column name of the numeric value to be formatted\n",
      "        d : int\n",
      "            the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "    \n",
      "    from_csv(col, schema, options={})\n",
      "        Parses a column containing a CSV string to a row with the specified schema.\n",
      "        Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in CSV format\n",
      "        schema :class:`~pyspark.sql.Column` or str\n",
      "            a string with schema in DDL format to use when parsing the CSV column.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1,2,3\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(a=1, b=2, c=3))]\n",
      "        >>> value = data[0][0]\n",
      "        >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
      "        >>> data = [(\"   abc\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> options = {'ignoreLeadingWhiteSpace': True}\n",
      "        >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(s='abc'))]\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        schema : :class:`DataType` or str\n",
      "            a StructType or ArrayType of StructType to use when parsing the json column.\n",
      "        \n",
      "            .. versionchanged:: 2.3\n",
      "                the DDL-formatted string is also supported for ``schema``.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=None))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        path : str\n",
      "            path to the json object to extract\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The list of columns should match with grouping columns exactly, or empty (means all\n",
      "        the grouping columns).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "    \n",
      "    hours(col)\n",
      "        Partition transform function: A transform for timestamps\n",
      "        to partition data into hours.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\n",
      "        ...     hours(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        fields : str\n",
      "            fields to extract\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, offset=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `default` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    lead(col, offset=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `default` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "    \n",
      "    length(col)\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
      "        [Row(height=5, spark_user=True)]\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : str\n",
      "            a string\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a Column of :class:`pyspark.sql.types.StringType`\n",
      "        pos : int, optional\n",
      "            start position (zero based)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string expression to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    map_concat(*cols)\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |map3                    |\n",
      "        +------------------------+\n",
      "        |{1 -> a, 2 -> b, 3 -> c}|\n",
      "        +------------------------+\n",
      "    \n",
      "    map_entries(col)\n",
      "        Collection function: Returns an unordered array of all entries in the given map.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_entries\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_entries(\"data\").alias(\"entries\")).show()\n",
      "        +----------------+\n",
      "        |         entries|\n",
      "        +----------------+\n",
      "        |[{1, a}, {2, b}]|\n",
      "        +----------------+\n",
      "    \n",
      "    map_filter(col, f)\n",
      "        Returns a map whose key-value pairs satisfy a predicate.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(map_filter(\n",
      "        ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |data_filtered             |\n",
      "        +--------------------------+\n",
      "        |{baz -> 32.0, foo -> 42.0}|\n",
      "        +--------------------------+\n",
      "    \n",
      "    map_from_arrays(col1, col2)\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of keys. All elements should not be null\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{2 -> a, 5 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_from_entries(col)\n",
      "        Collection function: Returns a map created from the given array of entries.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{1 -> a, 2 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_keys(col)\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "    \n",
      "    map_values(col)\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "    \n",
      "    map_zip_with(col1, col2, f)\n",
      "        Merge two given maps, key-wise into a single map using a function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
      "        ...     (\"id\", \"base\", \"ratio\")\n",
      "        ... )\n",
      "        >>> df.select(map_zip_with(\n",
      "        ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
      "        ... ).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |updated_data               |\n",
      "        +---------------------------+\n",
      "        |{SALES -> 16.8, IT -> 48.0}|\n",
      "        +---------------------------+\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "    \n",
      "    month(col)\n",
      "        Extract the month of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(month('dt').alias('month')).collect()\n",
      "        [Row(month=4)]\n",
      "    \n",
      "    months(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into months.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(\n",
      "        ...     months(\"ts\")\n",
      "        ... ).createOrReplace()  # doctest: +SKIP\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    months_between(date1, date2, roundOff=True)\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "        returns an integer (time of day will be ignored).\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "    \n",
      "    nth_value(col, offset, ignoreNulls=False)\n",
      "        Window function: returns the value that is the `offset`\\th row of the window frame\n",
      "        (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n",
      "        \n",
      "        It will return the `offset`\\th non-null value it sees when `ignoreNulls` is set to\n",
      "        true. If all values are null, then null is returned.\n",
      "        \n",
      "        This is equivalent to the nth_value function in SQL.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional\n",
      "            number of row to use as the value\n",
      "        ignoreNulls : bool, optional\n",
      "            indicates the Nth value should skip null in the\n",
      "            determination of which row to use\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            an integer\n",
      "    \n",
      "    overlay(src, replace, pos, len=-1)\n",
      "        Overlay the specified portion of `src` with `replace`,\n",
      "        starting from byte position `pos` of `src` and proceeding for `len` bytes.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).show()\n",
      "        +----------+\n",
      "        | overlayed|\n",
      "        +----------+\n",
      "        |SPARK_CORE|\n",
      "        +----------+\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    percentile_approx(col, percentage, accuracy=10000)\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        The value of percentage must be between 0.0 and 1.0.\n",
      "        \n",
      "        The accuracy parameter (default: 10000)\n",
      "        is a positive numeric literal which controls approximation accuracy at the cost of memory.\n",
      "        Higher value of accuracy yields better accuracy, 1.0/accuracy is the relative error\n",
      "        of the approximation.\n",
      "        \n",
      "        When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "        In this case, returns the approximate percentile array of column col\n",
      "        at the given percentage array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- quantiles: array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- median: double (nullable = true)\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    posexplode_outer(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|null|null| null|\n",
      "        |  3|      null|null|null| null|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|{x -> 1.0}|   0| foo|\n",
      "        |  1|{x -> 1.0}|   1| bar|\n",
      "        |  2|        {}|null|null|\n",
      "        |  3|      null|null|null|\n",
      "        +---+----------+----+----+\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in degrees\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "    \n",
      "    raise_error(errMsg)\n",
      "        Throws an exception with the provided error message.\n",
      "        \n",
      "        .. versionadded:: 3.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n",
      "        [Row(age=2, name='Alice', rand=2.4052597283576684),\n",
      "         Row(age=5, name='Bob', rand=2.3913904055683974)]\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('randn', randn(seed=42)).collect()\n",
      "        [Row(age=2, name='Alice', randn=1.1027054481455365),\n",
      "        Row(age=5, name='Bob', randn=0.7400395449950132)]\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "    \n",
      "    reverse(col)\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and\n",
      "        is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    schema_of_csv(csv, options={})\n",
      "        Parses a CSV string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        csv : :class:`~pyspark.sql.Column` or str\n",
      "            a CSV string or a foldable string column containing a CSV string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<`_c0`: INT, `_c1`: STRING>')]\n",
      "        >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<`_c0`: INT, `_c1`: STRING>')]\n",
      "    \n",
      "    schema_of_json(json, options={})\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        json : :class:`~pyspark.sql.Column` or str\n",
      "            a JSON string or a foldable string column containing a JSON string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the JSON datasource\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               It accepts `options` parameter to control schema inferring.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<`a`: BIGINT>')]\n",
      "        >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
      "        >>> df.select(schema.alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<`a`: BIGINT>')]\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "    \n",
      "    sequence(start, stop, step=None)\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "    \n",
      "    shuffle(col)\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "    \n",
      "    sinh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic sine of the given value,\n",
      "            as if computed by `java.lang.Math.sinh()`\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    slice(x, start, length)\n",
      "        Collection function: returns an array containing  all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the array to be sliced\n",
      "        start : :class:`~pyspark.sql.Column` or int\n",
      "            the starting index\n",
      "        length : :class:`~pyspark.sql.Column` or int\n",
      "            the length of the slice\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        asc : bool, optional\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is non deterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "    \n",
      "    split(str, pattern, limit=-1)\n",
      "        Splits str around matches of the given pattern.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a string expression to split\n",
      "        pattern : str\n",
      "            a string representing a regular expression. The regex string should be\n",
      "            a Java regular expression.\n",
      "        limit : int, optional\n",
      "            an integer which controls the number of times `pattern` is applied.\n",
      "        \n",
      "            * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                             resulting array's last entry will contain all input beyond the last\n",
      "                             matched pattern.\n",
      "            * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                              array can be of any size.\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "        [Row(s=['one', 'twoBthreeC'])]\n",
      "        >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "        [Row(s=['one', 'two', 'three', ''])]\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, set, str or :class:`~pyspark.sql.Column`\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "    \n",
      "    tanh(col)\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic tangent of the given value\n",
      "            as if computed by `java.lang.Math.tanh()`\n",
      "    \n",
      "    timestamp_seconds(col)\n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2008-12-25 07:30:00|\n",
      "        +-------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`degrees` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`radians` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_csv(col, options={})\n",
      "        Converts a column containing a :class:`StructType` into a CSV string.\n",
      "        Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct.\n",
      "        options: dict, optional\n",
      "            options to control converting. accepts the same options as the CSV datasource.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n",
      "        [Row(csv='2,Alice')]\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct, an array or a map.\n",
      "        options : dict, optional\n",
      "            options to control converting. accepts the same options as the JSON datasource.\n",
      "            Additionally the function supports the `pretty` option which enables\n",
      "            pretty JSON generation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            upported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4.0\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "    \n",
      "    transform(col, f)\n",
      "        Returns an array of elements after applying a transformation to each element in the input array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a function that is applied to each element of the input array.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
      "        >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n",
      "        +------------+\n",
      "        |     doubled|\n",
      "        +------------+\n",
      "        |[2, 4, 6, 8]|\n",
      "        +------------+\n",
      "        \n",
      "        >>> def alternate(x, i):\n",
      "        ...     return when(i % 2 == 0, x).otherwise(-x)\n",
      "        >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n",
      "        +--------------+\n",
      "        |    alternated|\n",
      "        +--------------+\n",
      "        |[1, -2, 3, -4]|\n",
      "        +--------------+\n",
      "    \n",
      "    transform_keys(col, f)\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new keys for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(transform_keys(\n",
      "        ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
      "        ... ).show(truncate=False)\n",
      "        +-------------------------+\n",
      "        |data_upper               |\n",
      "        +-------------------------+\n",
      "        |{BAR -> 2.0, FOO -> -2.0}|\n",
      "        +-------------------------+\n",
      "    \n",
      "    transform_values(col, f)\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new values for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
      "        >>> df.select(transform_values(\n",
      "        ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
      "        ... ).alias(\"new_data\")).show(truncate=False)\n",
      "        +---------------------------------------+\n",
      "        |new_data                               |\n",
      "        +---------------------------------------+\n",
      "        |{OPS -> 34.0, IT -> 20.0, SALES -> 2.0}|\n",
      "        +---------------------------------------+\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            python function if used as a standalone function\n",
      "        returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string expression to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased sample variance of\n",
      "        the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: alias for var_samp\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "        conditions.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`~pyspark.sql.Column`\n",
      "            a boolean :class:`~pyspark.sql.Column` expression.\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "    \n",
      "    xxhash64(*cols)\n",
      "        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n",
      "        and returns the result as a long column.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(xxhash64('a').alias('hash')).collect()\n",
      "        [Row(hash=4105715581806190027)]\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "    \n",
      "    years(col)\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into years.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     years(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    zip_with(left, right, f)\n",
      "        Merge two given arrays, element-wise, into a single array using a function.\n",
      "        If one array is shorter, nulls are appended at the end to match the length of the longer\n",
      "        array, before applying the function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a binary function ``(x1: Column, x2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |powers                     |\n",
      "        +---------------------------+\n",
      "        |[1.0, 9.0, 625.0, 262144.0]|\n",
      "        +---------------------------+\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n",
      "        +-----------------+\n",
      "        |            xs_ys|\n",
      "        +-----------------+\n",
      "        |[foo_1, bar_2, 3]|\n",
      "        +-----------------+\n",
      "\n",
      "FILE\n",
      "    /opt/spark3/python/pyspark/sql/functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae029e83-85f0-40c7-98aa-6d1cfa1ab533",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|2022-02-26 03:16:17.383|2022|2    |26        |3   |16    |17    |\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_timestamp().alias('current_timestamp'), \n",
    "    year(current_timestamp()).alias('year'),\n",
    "    month(current_timestamp()).alias('month'),\n",
    "    dayofmonth(current_timestamp()).alias('dayofmonth'),\n",
    "    hour(current_timestamp()).alias('hour'),\n",
    "    minute(current_timestamp()).alias('minute'),\n",
    "    second(current_timestamp()).alias('second')\n",
    ").show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f87afae9-1850-43d6-a668-452e352a30a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15 Date and Time Extract Functions",
   "notebookOrigID": 456917195748651,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3 (local)",
   "language": "python",
   "name": "pyspark3local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
