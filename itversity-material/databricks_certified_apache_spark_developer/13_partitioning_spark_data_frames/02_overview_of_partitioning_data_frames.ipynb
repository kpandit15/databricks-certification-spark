{"cells":[{"cell_type":"code","source":["df = spark.read.json('/public/retail_db_json/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b0b639-f11d-4248-9d73-feaf46fcd9dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.write"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a97d1f2-ab5c-4d7b-bbc2-4c8f9231ee35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: &lt;pyspark.sql.readwriter.DataFrameWriter at 0x7f67b6f58df0&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &lt;pyspark.sql.readwriter.DataFrameWriter at 0x7f67b6f58df0&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(df.write.partitionBy)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"874f135e-a0a7-4c64-b434-8083d0e601cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method partitionBy in module pyspark.sql.readwriter:\n\npartitionBy(*cols) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Partitions the output by the given columns on the file system.\n    \n    If specified, the output is laid out on the file system similar\n    to Hive&#39;s partitioning scheme.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    cols : str or list\n        name of columns\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.partitionBy(&#39;year&#39;, &#39;month&#39;).parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method partitionBy in module pyspark.sql.readwriter:\n\npartitionBy(*cols) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Partitions the output by the given columns on the file system.\n    \n    If specified, the output is laid out on the file system similar\n    to Hive&#39;s partitioning scheme.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    cols : str or list\n        name of columns\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.partitionBy(&#39;year&#39;, &#39;month&#39;).parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# json does not have keyword argument related to partitioning\nhelp(df.write.json)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fa0c6ee-4cff-4d7c-8add-09f17c04da82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method json in module pyspark.sql.readwriter:\n\njson(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in JSON format\n    (`JSON Lines text format or newline-delimited JSON &lt;http://jsonlines.org/&gt;`_) at the\n    specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n        snappy and deflate).\n    dateFormat : str, optional\n        sets the string that indicates a date format. Custom date formats\n        follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to date type. If None is set, it uses the\n        default value, ``yyyy-MM-dd``.\n    timestampFormat : str, optional\n        sets the string that indicates a timestamp format.\n        Custom date formats follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to timestamp type. If None is set, it uses the\n        default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]``.\n    encoding : str, optional\n        specifies encoding (charset) of saved json files. If None is set,\n        the default UTF-8 charset will be used.\n    lineSep : str, optional\n        defines the line separator that should be used for writing. If None is\n        set, it uses the default value, ``\\n``.\n    ignoreNullFields : str or bool, optional\n        Whether to ignore null fields when generating JSON objects.\n        If None is set, it uses the default value, ``true``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.json(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method json in module pyspark.sql.readwriter:\n\njson(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in JSON format\n    (`JSON Lines text format or newline-delimited JSON &lt;http://jsonlines.org/&gt;`_) at the\n    specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n        snappy and deflate).\n    dateFormat : str, optional\n        sets the string that indicates a date format. Custom date formats\n        follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to date type. If None is set, it uses the\n        default value, ``yyyy-MM-dd``.\n    timestampFormat : str, optional\n        sets the string that indicates a timestamp format.\n        Custom date formats follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to timestamp type. If None is set, it uses the\n        default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]``.\n    encoding : str, optional\n        specifies encoding (charset) of saved json files. If None is set,\n        the default UTF-8 charset will be used.\n    lineSep : str, optional\n        defines the line separator that should be used for writing. If None is\n        set, it uses the default value, ``\\n``.\n    ignoreNullFields : str or bool, optional\n        Whether to ignore null fields when generating JSON objects.\n        If None is set, it uses the default value, ``true``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.json(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# parquet have keyword argument partitionBy\nhelp(df.write.parquet)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14574b5b-91f0-43fd-8e87-1c909238ca82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method parquet in module pyspark.sql.readwriter:\n\nparquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    partitionBy : str or list, optional\n        names of partitioning columns\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n        lzo, brotli, lz4, and zstd). This will override\n        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n        value specified in ``spark.sql.parquet.compression.codec``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method parquet in module pyspark.sql.readwriter:\n\nparquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str\n        the path in any Hadoop supported file system\n    mode : str, optional\n        specifies the behavior of the save operation when data already exists.\n    \n        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n        * ``overwrite``: Overwrite existing data.\n        * ``ignore``: Silently ignore this operation if data already exists.\n        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n    partitionBy : str or list, optional\n        names of partitioning columns\n    compression : str, optional\n        compression codec to use when saving to file. This can be one of the\n        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n        lzo, brotli, lz4, and zstd). This will override\n        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n        value specified in ``spark.sql.parquet.compression.codec``.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.write.parquet(os.path.join(tempfile.mkdtemp(), &#39;data&#39;))\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38156da4-59a4-46fa-b575-c11f0d4d10ea"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02 Overview of Partitioning Data Frames","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195749323}},"nbformat":4,"nbformat_minor":0}