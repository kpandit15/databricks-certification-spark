{"cells":[{"cell_type":"code","source":["help(spark.udf.register)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eaca4c3-367e-4dc7-9b5a-c3641478f307"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method register in module pyspark.sql.udf:\n\nregister(name, f, returnType=None) method of pyspark.sql.udf.UDFRegistration instance\n    Register a Python function (including lambda function) or a user-defined function\n    as a SQL function.\n    \n    .. versionadded:: 1.3.1\n    \n    Parameters\n    ----------\n    name : str,\n        name of the user-defined function in SQL statements.\n    f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n        a Python function, or a user-defined function. The user-defined function can\n        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n        :meth:`pyspark.sql.functions.pandas_udf`.\n    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n        the return type of the registered user-defined function. The value can\n        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        `returnType` can be optionally specified when `f` is a Python function but not\n        when `f` is a user-defined function. Please see the examples below.\n    \n    Returns\n    -------\n    function\n        a user-defined function\n    \n    Notes\n    -----\n    To register a nondeterministic Python function, users need to first build\n    a nondeterministic user-defined function for the Python function and then register it\n    as a SQL function.\n    \n    Examples\n    --------\n    1. When `f` is a Python function:\n    \n        `returnType` defaults to string type and can be optionally specified. The produced\n        object must match the specified type. In this case, this API works as if\n        `register(name, f, returnType=StringType())`.\n    \n        &gt;&gt;&gt; strlen = spark.udf.register(&#34;stringLengthString&#34;, lambda x: len(x))\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthString(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthString(test)=&#39;4&#39;)]\n    \n        &gt;&gt;&gt; spark.sql(&#34;SELECT &#39;foo&#39; AS text&#34;).select(strlen(&#34;text&#34;)).collect()\n        [Row(stringLengthString(text)=&#39;3&#39;)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n    2. When `f` is a user-defined function (from Spark 2.3.0):\n    \n        Spark uses the return type of the given user-defined function as the return type of\n        the registered user-defined function. `returnType` should not be specified.\n        In this case, this API works as if `register(name, f)`.\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;slen&#34;, slen)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT slen(&#39;test&#39;)&#34;).collect()\n        [Row(slen(test)=4)]\n    \n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n        &gt;&gt;&gt; new_random_udf = spark.udf.register(&#34;random_udf&#34;, random_udf)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT random_udf()&#34;).collect()  # doctest: +SKIP\n        [Row(random_udf()=82)]\n    \n        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP\n        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def add_one(s: pd.Series) -&gt; pd.Series:\n        ...     return s + 1\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;add_one&#34;, add_one)  # doctest: +SKIP\n        &gt;&gt;&gt; spark.sql(&#34;SELECT add_one(id) FROM range(3)&#34;).collect()  # doctest: +SKIP\n        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n    \n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def sum_udf(v: pd.Series) -&gt; int:\n        ...     return v.sum()\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;sum_udf&#34;, sum_udf)  # doctest: +SKIP\n        &gt;&gt;&gt; q = &#34;SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2&#34;\n        &gt;&gt;&gt; spark.sql(q).collect()  # doctest: +SKIP\n        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method register in module pyspark.sql.udf:\n\nregister(name, f, returnType=None) method of pyspark.sql.udf.UDFRegistration instance\n    Register a Python function (including lambda function) or a user-defined function\n    as a SQL function.\n    \n    .. versionadded:: 1.3.1\n    \n    Parameters\n    ----------\n    name : str,\n        name of the user-defined function in SQL statements.\n    f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n        a Python function, or a user-defined function. The user-defined function can\n        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n        :meth:`pyspark.sql.functions.pandas_udf`.\n    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n        the return type of the registered user-defined function. The value can\n        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        `returnType` can be optionally specified when `f` is a Python function but not\n        when `f` is a user-defined function. Please see the examples below.\n    \n    Returns\n    -------\n    function\n        a user-defined function\n    \n    Notes\n    -----\n    To register a nondeterministic Python function, users need to first build\n    a nondeterministic user-defined function for the Python function and then register it\n    as a SQL function.\n    \n    Examples\n    --------\n    1. When `f` is a Python function:\n    \n        `returnType` defaults to string type and can be optionally specified. The produced\n        object must match the specified type. In this case, this API works as if\n        `register(name, f, returnType=StringType())`.\n    \n        &gt;&gt;&gt; strlen = spark.udf.register(&#34;stringLengthString&#34;, lambda x: len(x))\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthString(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthString(test)=&#39;4&#39;)]\n    \n        &gt;&gt;&gt; spark.sql(&#34;SELECT &#39;foo&#39; AS text&#34;).select(strlen(&#34;text&#34;)).collect()\n        [Row(stringLengthString(text)=&#39;3&#39;)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n    2. When `f` is a user-defined function (from Spark 2.3.0):\n    \n        Spark uses the return type of the given user-defined function as the return type of\n        the registered user-defined function. `returnType` should not be specified.\n        In this case, this API works as if `register(name, f)`.\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;slen&#34;, slen)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT slen(&#39;test&#39;)&#34;).collect()\n        [Row(slen(test)=4)]\n    \n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n        &gt;&gt;&gt; new_random_udf = spark.udf.register(&#34;random_udf&#34;, random_udf)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT random_udf()&#34;).collect()  # doctest: +SKIP\n        [Row(random_udf()=82)]\n    \n        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP\n        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def add_one(s: pd.Series) -&gt; pd.Series:\n        ...     return s + 1\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;add_one&#34;, add_one)  # doctest: +SKIP\n        &gt;&gt;&gt; spark.sql(&#34;SELECT add_one(id) FROM range(3)&#34;).collect()  # doctest: +SKIP\n        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n    \n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def sum_udf(v: pd.Series) -&gt; int:\n        ...     return v.sum()\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;sum_udf&#34;, sum_udf)  # doctest: +SKIP\n        &gt;&gt;&gt; q = &#34;SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2&#34;\n        &gt;&gt;&gt; spark.sql(q).collect()  # doctest: +SKIP\n        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.json('/public/retail_db_json/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d48c4520-1c94-43f3-9c8c-1f7826ac25c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ee47c85-3ba7-41a4-bb4a-23cceb357ea5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\norder_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n            11599|2013-07-25 00:00:...|       1|         CLOSED|\n              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n             8827|2013-07-25 00:00:...|       4|         CLOSED|\n            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n             1837|2013-07-25 00:00:...|      12|         CLOSED|\n             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n             1205|2013-07-25 00:00:...|      18|         CLOSED|\n             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dc = spark.udf.register('date_convert', lambda d: int(d[:10].replace('-', '')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fe52565-62e9-4da6-aa53-b557d43ed227"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb896ccb-9f79-4dfd-aefb-6f58722b9086"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: &lt;function __main__.&lt;lambda&gt;(d)&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: &lt;function __main__.&lt;lambda&gt;(d)&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(dc('order_date').alias('order_date')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c87ef2e9-0fdf-4578-b5da-703aeed6edf0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+\n|order_date|\n+----------+\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n|  20130725|\n+----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\norder_date|\n+----------+\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n  20130725|\n+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.filter(dc('order_date') == 20140101).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3ed2137-e573-4870-9ab5-fc67a95e4867"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\norder_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n             9084|2014-01-01 00:00:...|   25878|        PENDING|\n             5118|2014-01-01 00:00:...|   25879|        PENDING|\n            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n            11764|2014-01-01 00:00:...|   25883|        PENDING|\n             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n             7253|2014-01-01 00:00:...|   25885|        PENDING|\n             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n            10062|2014-01-01 00:00:...|   25887|        PENDING|\n             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n             2581|2014-01-01 00:00:...|   25890|        PENDING|\n             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df. \\\n    groupBy(dc('order_date').alias('order_date')). \\\n    count(). \\\n    withColumnRenamed('count', 'order_count'). \\\n    show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96719063-aac9-45e6-897d-827397bbbeca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+-----------+\n|order_date|order_count|\n+----------+-----------+\n|  20130919|        206|\n|  20140303|        266|\n|  20140202|        192|\n|  20140310|        235|\n|  20130809|        125|\n|  20130817|        253|\n|  20131015|        174|\n|  20140114|        209|\n|  20131029|        128|\n|  20140130|        254|\n|  20130824|        265|\n|  20130913|        103|\n|  20130914|        276|\n|  20130825|        200|\n|  20131031|        208|\n|  20140304|        257|\n|  20130731|        252|\n|  20130730|        227|\n|  20131116|        120|\n|  20131213|        135|\n+----------+-----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+\norder_date|order_count|\n+----------+-----------+\n  20130919|        206|\n  20140303|        266|\n  20140202|        192|\n  20140310|        235|\n  20130809|        125|\n  20130817|        253|\n  20131015|        174|\n  20140114|        209|\n  20131029|        128|\n  20140130|        254|\n  20130824|        265|\n  20130913|        103|\n  20130914|        276|\n  20130825|        200|\n  20131031|        208|\n  20140304|        257|\n  20130731|        252|\n  20130730|        227|\n  20131116|        120|\n  20131213|        135|\n+----------+-----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d25a2e5-d7bd-47b4-8544-7e9fcd57abf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04 Using Spark UDFs as part of Data Frame APIs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195748950}},"nbformat":4,"nbformat_minor":0}