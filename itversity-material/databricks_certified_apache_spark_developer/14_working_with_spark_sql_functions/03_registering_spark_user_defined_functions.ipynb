{"cells":[{"cell_type":"markdown","source":["Here are the steps we need to follow to develop and use Spark User Defined Functions.\n* Develop required logic using Python as programming language.\n* Register the function using `spark.udf.register`. Also assign it to a variable.\n* Variable can be used as part of Data Frame APIs such as `select`, `filter`, etc.\n* When we register, we register with a name. That name can be used as part of `selectExpr` or as part of Spark SQL queries using `spark.sql`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dbfb900-9c2b-4e3a-9312-c1cdee4f726b"}}},{"cell_type":"code","source":["help(spark.udf.register)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5eaca4c3-367e-4dc7-9b5a-c3641478f307"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method register in module pyspark.sql.udf:\n\nregister(name, f, returnType=None) method of pyspark.sql.udf.UDFRegistration instance\n    Register a Python function (including lambda function) or a user-defined function\n    as a SQL function.\n    \n    .. versionadded:: 1.3.1\n    \n    Parameters\n    ----------\n    name : str,\n        name of the user-defined function in SQL statements.\n    f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n        a Python function, or a user-defined function. The user-defined function can\n        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n        :meth:`pyspark.sql.functions.pandas_udf`.\n    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n        the return type of the registered user-defined function. The value can\n        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        `returnType` can be optionally specified when `f` is a Python function but not\n        when `f` is a user-defined function. Please see the examples below.\n    \n    Returns\n    -------\n    function\n        a user-defined function\n    \n    Notes\n    -----\n    To register a nondeterministic Python function, users need to first build\n    a nondeterministic user-defined function for the Python function and then register it\n    as a SQL function.\n    \n    Examples\n    --------\n    1. When `f` is a Python function:\n    \n        `returnType` defaults to string type and can be optionally specified. The produced\n        object must match the specified type. In this case, this API works as if\n        `register(name, f, returnType=StringType())`.\n    \n        &gt;&gt;&gt; strlen = spark.udf.register(&#34;stringLengthString&#34;, lambda x: len(x))\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthString(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthString(test)=&#39;4&#39;)]\n    \n        &gt;&gt;&gt; spark.sql(&#34;SELECT &#39;foo&#39; AS text&#34;).select(strlen(&#34;text&#34;)).collect()\n        [Row(stringLengthString(text)=&#39;3&#39;)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n    2. When `f` is a user-defined function (from Spark 2.3.0):\n    \n        Spark uses the return type of the given user-defined function as the return type of\n        the registered user-defined function. `returnType` should not be specified.\n        In this case, this API works as if `register(name, f)`.\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;slen&#34;, slen)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT slen(&#39;test&#39;)&#34;).collect()\n        [Row(slen(test)=4)]\n    \n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n        &gt;&gt;&gt; new_random_udf = spark.udf.register(&#34;random_udf&#34;, random_udf)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT random_udf()&#34;).collect()  # doctest: +SKIP\n        [Row(random_udf()=82)]\n    \n        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP\n        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def add_one(s: pd.Series) -&gt; pd.Series:\n        ...     return s + 1\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;add_one&#34;, add_one)  # doctest: +SKIP\n        &gt;&gt;&gt; spark.sql(&#34;SELECT add_one(id) FROM range(3)&#34;).collect()  # doctest: +SKIP\n        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n    \n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def sum_udf(v: pd.Series) -&gt; int:\n        ...     return v.sum()\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;sum_udf&#34;, sum_udf)  # doctest: +SKIP\n        &gt;&gt;&gt; q = &#34;SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2&#34;\n        &gt;&gt;&gt; spark.sql(q).collect()  # doctest: +SKIP\n        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method register in module pyspark.sql.udf:\n\nregister(name, f, returnType=None) method of pyspark.sql.udf.UDFRegistration instance\n    Register a Python function (including lambda function) or a user-defined function\n    as a SQL function.\n    \n    .. versionadded:: 1.3.1\n    \n    Parameters\n    ----------\n    name : str,\n        name of the user-defined function in SQL statements.\n    f : function, :meth:`pyspark.sql.functions.udf` or :meth:`pyspark.sql.functions.pandas_udf`\n        a Python function, or a user-defined function. The user-defined function can\n        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n        :meth:`pyspark.sql.functions.pandas_udf`.\n    returnType : :class:`pyspark.sql.types.DataType` or str, optional\n        the return type of the registered user-defined function. The value can\n        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n        `returnType` can be optionally specified when `f` is a Python function but not\n        when `f` is a user-defined function. Please see the examples below.\n    \n    Returns\n    -------\n    function\n        a user-defined function\n    \n    Notes\n    -----\n    To register a nondeterministic Python function, users need to first build\n    a nondeterministic user-defined function for the Python function and then register it\n    as a SQL function.\n    \n    Examples\n    --------\n    1. When `f` is a Python function:\n    \n        `returnType` defaults to string type and can be optionally specified. The produced\n        object must match the specified type. In this case, this API works as if\n        `register(name, f, returnType=StringType())`.\n    \n        &gt;&gt;&gt; strlen = spark.udf.register(&#34;stringLengthString&#34;, lambda x: len(x))\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthString(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthString(test)=&#39;4&#39;)]\n    \n        &gt;&gt;&gt; spark.sql(&#34;SELECT &#39;foo&#39; AS text&#34;).select(strlen(&#34;text&#34;)).collect()\n        [Row(stringLengthString(text)=&#39;3&#39;)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;stringLengthInt&#34;, lambda x: len(x), IntegerType())\n        &gt;&gt;&gt; spark.sql(&#34;SELECT stringLengthInt(&#39;test&#39;)&#34;).collect()\n        [Row(stringLengthInt(test)=4)]\n    \n    2. When `f` is a user-defined function (from Spark 2.3.0):\n    \n        Spark uses the return type of the given user-defined function as the return type of\n        the registered user-defined function. `returnType` should not be specified.\n        In this case, this API works as if `register(name, f)`.\n    \n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; slen = udf(lambda s: len(s), IntegerType())\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;slen&#34;, slen)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT slen(&#39;test&#39;)&#34;).collect()\n        [Row(slen(test)=4)]\n    \n        &gt;&gt;&gt; import random\n        &gt;&gt;&gt; from pyspark.sql.functions import udf\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n        &gt;&gt;&gt; new_random_udf = spark.udf.register(&#34;random_udf&#34;, random_udf)\n        &gt;&gt;&gt; spark.sql(&#34;SELECT random_udf()&#34;).collect()  # doctest: +SKIP\n        [Row(random_udf()=82)]\n    \n        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP\n        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf\n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def add_one(s: pd.Series) -&gt; pd.Series:\n        ...     return s + 1\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;add_one&#34;, add_one)  # doctest: +SKIP\n        &gt;&gt;&gt; spark.sql(&#34;SELECT add_one(id) FROM range(3)&#34;).collect()  # doctest: +SKIP\n        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n    \n        &gt;&gt;&gt; @pandas_udf(&#34;integer&#34;)  # doctest: +SKIP\n        ... def sum_udf(v: pd.Series) -&gt; int:\n        ...     return v.sum()\n        ...\n        &gt;&gt;&gt; _ = spark.udf.register(&#34;sum_udf&#34;, sum_udf)  # doctest: +SKIP\n        &gt;&gt;&gt; q = &#34;SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2&#34;\n        &gt;&gt;&gt; spark.sql(q).collect()  # doctest: +SKIP\n        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03 Registering Spark User Defined Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195748963}},"nbformat":4,"nbformat_minor":0}