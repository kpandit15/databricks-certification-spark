{"cells":[{"cell_type":"markdown","source":["* We can read the data from CSV files into Spark Data Frame using multiple approaches.\n* Approach 1: `spark.read.csv('path_to_folder')`\n* Approach 2: `spark.read.format('csv').load('path_to_folder')`\n* We can explicitly specify the schema as `string` or using `StructType`.\n* We can also read the data which is delimited or separated by other characters than comma.\n* If the files have header we can create the Data Frame with schema by using options such as `header` and `inferSchema`. It will pick column names from the header while data types will be inferred based on the data.\n* If the files does not have header we can create the Data Frame with schema by passing column names using `toDF` and by using `inferSchema` option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb0df909-df4b-4584-adcd-74459f6864f9"}}},{"cell_type":"code","source":["# Default behavior\n# It will delimit the data using comma as separator\n# Column names will be system generated\n# All the fields will be of type strings\norders = spark.read.csv('/public/retail_db/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4520988-7f01-4f70-af96-67495a062c15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["orders.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7783562f-dba8-441b-ab8c-8f2314ad9abb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: [&#39;_c0&#39;, &#39;_c1&#39;, &#39;_c2&#39;, &#39;_c3&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: [&#39;_c0&#39;, &#39;_c1&#39;, &#39;_c2&#39;, &#39;_c3&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["orders.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5835ba3-44b8-411c-816a-4c82c28294df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: [(&#39;_c0&#39;, &#39;string&#39;), (&#39;_c1&#39;, &#39;string&#39;), (&#39;_c2&#39;, &#39;string&#39;), (&#39;_c3&#39;, &#39;string&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [(&#39;_c0&#39;, &#39;string&#39;), (&#39;_c1&#39;, &#39;string&#39;), (&#39;_c2&#39;, &#39;string&#39;), (&#39;_c3&#39;, &#39;string&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37701bc2-c587-4ba2-8019-0cac5e8fcc17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"07 Reading Data from CSV files into Spark Data Frame","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195748364}},"nbformat":4,"nbformat_minor":0}