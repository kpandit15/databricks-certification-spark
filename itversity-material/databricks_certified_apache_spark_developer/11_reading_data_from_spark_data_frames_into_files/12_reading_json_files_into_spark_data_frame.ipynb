{"cells":[{"cell_type":"code","source":["help(spark.read.json)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2ed4710-0c33-49dd-b14f-e49b1496084f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method json in module pyspark.sql.readwriter:\n\njson(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None, allowNonNumericNumbers=None, modifiedBefore=None, modifiedAfter=None) method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads JSON files and returns the results as a :class:`DataFrame`.\n    \n    `JSON Lines &lt;http://jsonlines.org/&gt;`_ (newline-delimited JSON) is supported by default.\n    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n    \n    If the ``schema`` parameter is not specified, this function goes\n    through the input once to determine the input schema.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str, list or :class:`RDD`\n        string represents path to the JSON dataset, or a list of paths,\n        or RDD of Strings storing JSON objects.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    primitivesAsString : str or bool, optional\n        infers all primitive values as a string type. If None is set,\n        it uses the default value, ``false``.\n    prefersDecimal : str or bool, optional\n        infers all floating-point values as a decimal type. If the values\n        do not fit in decimal, then it infers them as doubles. If None is\n        set, it uses the default value, ``false``.\n    allowComments : str or bool, optional\n        ignores Java/C++ style comment in JSON records. If None is set,\n        it uses the default value, ``false``.\n    allowUnquotedFieldNames : str or bool, optional\n        allows unquoted JSON field names. If None is set,\n        it uses the default value, ``false``.\n    allowSingleQuotes : str or bool, optional\n        allows single quotes in addition to double quotes. If None is\n        set, it uses the default value, ``true``.\n    allowNumericLeadingZero : str or bool, optional\n        allows leading zeros in numbers (e.g. 00012). If None is\n        set, it uses the default value, ``false``.\n    allowBackslashEscapingAnyCharacter : str or bool, optional\n        allows accepting quoting of all character\n        using backslash quoting mechanism. If None is\n        set, it uses the default value, ``false``.\n    mode : str, optional\n        allows a mode for dealing with corrupt records during parsing. If None is\n                 set, it uses the default value, ``PERMISSIVE``.\n    \n        * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string               into a field configured by ``columnNameOfCorruptRecord``, and sets malformed               fields to ``null``. To keep corrupt records, an user can set a string type               field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a               schema does not have the field, it drops corrupt records during parsing.               When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``               field in an output schema.\n        *  ``DROPMALFORMED``: ignores the whole corrupted records.\n        *  ``FAILFAST``: throws an exception when it meets corrupted records.\n    \n    columnNameOfCorruptRecord: str, optional\n        allows renaming the new field having malformed string\n        created by ``PERMISSIVE`` mode. This overrides\n        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n        it uses the value specified in\n        ``spark.sql.columnNameOfCorruptRecord``.\n    dateFormat : str, optional\n        sets the string that indicates a date format. Custom date formats\n        follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to date type. If None is set, it uses the\n        default value, ``yyyy-MM-dd``.\n    timestampFormat : str, optional\n        sets the string that indicates a timestamp format.\n        Custom date formats follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to timestamp type. If None is set, it uses the\n        default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]``.\n    multiLine : str or bool, optional\n        parse one record, which may span multiple lines, per file. If None is\n        set, it uses the default value, ``false``.\n    allowUnquotedControlChars : str or bool, optional\n        allows JSON Strings to contain unquoted control\n        characters (ASCII characters with value less than 32,\n        including tab and line feed characters) or not.\n    encoding : str or bool, optional\n        allows to forcibly set one of standard basic or extended encoding for\n        the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n        the encoding of input JSON will be detected automatically\n        when the multiLine option is set to ``true``.\n    lineSep : str, optional\n        defines the line separator that should be used for parsing. If None is\n        set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n    samplingRatio : str or float, optional\n        defines fraction of input JSON objects used for schema inferring.\n        If None is set, it uses the default value, ``1.0``.\n    dropFieldIfAllNull : str or bool, optional\n        whether to ignore column of all null values or empty\n        array/struct during schema inference. If None is set, it\n        uses the default value, ``false``.\n    locale : str, optional\n        sets a locale as language tag in IETF BCP 47 format. If None is set,\n        it uses the default value, ``en-US``. For instance, ``locale`` is used while\n        parsing dates and timestamps.\n    pathGlobFilter : str or bool, optional\n        an optional glob pattern to only include files with paths matching\n        the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n        It does not change the behavior of\n        `partition discovery &lt;https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery&gt;`_.  # noqa\n    recursiveFileLookup : str or bool, optional\n        recursively scan a directory for files. Using this option\n        disables\n        `partition discovery &lt;https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery&gt;`_.  # noqa\n    allowNonNumericNumbers : str or bool\n        allows JSON parser to recognize set of &#34;Not-a-Number&#34; (NaN)\n        tokens as legal floating number values. If None is set,\n        it uses the default value, ``true``.\n    \n            * ``+INF``: for positive infinity, as well as alias of\n                        ``+Infinity`` and ``Infinity``.\n            *  ``-INF``: for negative infinity, alias ``-Infinity``.\n            *  ``NaN``: for other not-a-numbers, like result of division by zero.\n    modifiedBefore : an optional timestamp to only include files with\n        modification times occurring before the specified time. The provided timestamp\n        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n    modifiedAfter : an optional timestamp to only include files with\n        modification times occurring after the specified time. The provided timestamp\n        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n    \n    \n    Examples\n    --------\n    &gt;&gt;&gt; df1 = spark.read.json(&#39;python/test_support/sql/people.json&#39;)\n    &gt;&gt;&gt; df1.dtypes\n    [(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]\n    &gt;&gt;&gt; rdd = sc.textFile(&#39;python/test_support/sql/people.json&#39;)\n    &gt;&gt;&gt; df2 = spark.read.json(rdd)\n    &gt;&gt;&gt; df2.dtypes\n    [(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method json in module pyspark.sql.readwriter:\n\njson(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None, allowNonNumericNumbers=None, modifiedBefore=None, modifiedAfter=None) method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads JSON files and returns the results as a :class:`DataFrame`.\n    \n    `JSON Lines &lt;http://jsonlines.org/&gt;`_ (newline-delimited JSON) is supported by default.\n    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n    \n    If the ``schema`` parameter is not specified, this function goes\n    through the input once to determine the input schema.\n    \n    .. versionadded:: 1.4.0\n    \n    Parameters\n    ----------\n    path : str, list or :class:`RDD`\n        string represents path to the JSON dataset, or a list of paths,\n        or RDD of Strings storing JSON objects.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema or\n        a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    primitivesAsString : str or bool, optional\n        infers all primitive values as a string type. If None is set,\n        it uses the default value, ``false``.\n    prefersDecimal : str or bool, optional\n        infers all floating-point values as a decimal type. If the values\n        do not fit in decimal, then it infers them as doubles. If None is\n        set, it uses the default value, ``false``.\n    allowComments : str or bool, optional\n        ignores Java/C++ style comment in JSON records. If None is set,\n        it uses the default value, ``false``.\n    allowUnquotedFieldNames : str or bool, optional\n        allows unquoted JSON field names. If None is set,\n        it uses the default value, ``false``.\n    allowSingleQuotes : str or bool, optional\n        allows single quotes in addition to double quotes. If None is\n        set, it uses the default value, ``true``.\n    allowNumericLeadingZero : str or bool, optional\n        allows leading zeros in numbers (e.g. 00012). If None is\n        set, it uses the default value, ``false``.\n    allowBackslashEscapingAnyCharacter : str or bool, optional\n        allows accepting quoting of all character\n        using backslash quoting mechanism. If None is\n        set, it uses the default value, ``false``.\n    mode : str, optional\n        allows a mode for dealing with corrupt records during parsing. If None is\n                 set, it uses the default value, ``PERMISSIVE``.\n    \n        * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string               into a field configured by ``columnNameOfCorruptRecord``, and sets malformed               fields to ``null``. To keep corrupt records, an user can set a string type               field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a               schema does not have the field, it drops corrupt records during parsing.               When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``               field in an output schema.\n        *  ``DROPMALFORMED``: ignores the whole corrupted records.\n        *  ``FAILFAST``: throws an exception when it meets corrupted records.\n    \n    columnNameOfCorruptRecord: str, optional\n        allows renaming the new field having malformed string\n        created by ``PERMISSIVE`` mode. This overrides\n        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n        it uses the value specified in\n        ``spark.sql.columnNameOfCorruptRecord``.\n    dateFormat : str, optional\n        sets the string that indicates a date format. Custom date formats\n        follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to date type. If None is set, it uses the\n        default value, ``yyyy-MM-dd``.\n    timestampFormat : str, optional\n        sets the string that indicates a timestamp format.\n        Custom date formats follow the formats at\n        `datetime pattern &lt;https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html&gt;`_.  # noqa\n        This applies to timestamp type. If None is set, it uses the\n        default value, ``yyyy-MM-dd&#39;T&#39;HH:mm:ss[.SSS][XXX]``.\n    multiLine : str or bool, optional\n        parse one record, which may span multiple lines, per file. If None is\n        set, it uses the default value, ``false``.\n    allowUnquotedControlChars : str or bool, optional\n        allows JSON Strings to contain unquoted control\n        characters (ASCII characters with value less than 32,\n        including tab and line feed characters) or not.\n    encoding : str or bool, optional\n        allows to forcibly set one of standard basic or extended encoding for\n        the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n        the encoding of input JSON will be detected automatically\n        when the multiLine option is set to ``true``.\n    lineSep : str, optional\n        defines the line separator that should be used for parsing. If None is\n        set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n    samplingRatio : str or float, optional\n        defines fraction of input JSON objects used for schema inferring.\n        If None is set, it uses the default value, ``1.0``.\n    dropFieldIfAllNull : str or bool, optional\n        whether to ignore column of all null values or empty\n        array/struct during schema inference. If None is set, it\n        uses the default value, ``false``.\n    locale : str, optional\n        sets a locale as language tag in IETF BCP 47 format. If None is set,\n        it uses the default value, ``en-US``. For instance, ``locale`` is used while\n        parsing dates and timestamps.\n    pathGlobFilter : str or bool, optional\n        an optional glob pattern to only include files with paths matching\n        the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n        It does not change the behavior of\n        `partition discovery &lt;https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery&gt;`_.  # noqa\n    recursiveFileLookup : str or bool, optional\n        recursively scan a directory for files. Using this option\n        disables\n        `partition discovery &lt;https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery&gt;`_.  # noqa\n    allowNonNumericNumbers : str or bool\n        allows JSON parser to recognize set of &#34;Not-a-Number&#34; (NaN)\n        tokens as legal floating number values. If None is set,\n        it uses the default value, ``true``.\n    \n            * ``+INF``: for positive infinity, as well as alias of\n                        ``+Infinity`` and ``Infinity``.\n            *  ``-INF``: for negative infinity, alias ``-Infinity``.\n            *  ``NaN``: for other not-a-numbers, like result of division by zero.\n    modifiedBefore : an optional timestamp to only include files with\n        modification times occurring before the specified time. The provided timestamp\n        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n    modifiedAfter : an optional timestamp to only include files with\n        modification times occurring after the specified time. The provided timestamp\n        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n    \n    \n    Examples\n    --------\n    &gt;&gt;&gt; df1 = spark.read.json(&#39;python/test_support/sql/people.json&#39;)\n    &gt;&gt;&gt; df1.dtypes\n    [(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]\n    &gt;&gt;&gt; rdd = sc.textFile(&#39;python/test_support/sql/people.json&#39;)\n    &gt;&gt;&gt; df2 = spark.read.json(rdd)\n    &gt;&gt;&gt; df2.dtypes\n    [(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Schema will be inferred by default\ndf = spark.read.json('/public/retail_db_json/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf338d25-030d-4782-afb2-826b4947436a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format('json').load('/public/retail_db_json/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ada62a-f50e-4c18-8a5a-0d7ccbc3037f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.inputFiles()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76e553d4-c6ef-4697-b11a-3f9237511ee5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [&#39;dbfs:/public/retail_db_json/orders/part-r-00000-990f5773-9005-49ba-b670-631286032674&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [&#39;dbfs:/public/retail_db_json/orders/part-r-00000-990f5773-9005-49ba-b670-631286032674&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Data type for order_id as well as order_customer_id is inferred as bigint\n# Data type for order_date is inferred as string\ndf.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f30b7c0b-dbf4-4ff5-91ba-603f6ba8eb45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: [(&#39;order_customer_id&#39;, &#39;bigint&#39;),\n (&#39;order_date&#39;, &#39;string&#39;),\n (&#39;order_id&#39;, &#39;bigint&#39;),\n (&#39;order_status&#39;, &#39;string&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [(&#39;order_customer_id&#39;, &#39;bigint&#39;),\n (&#39;order_date&#39;, &#39;string&#39;),\n (&#39;order_id&#39;, &#39;bigint&#39;),\n (&#39;order_status&#39;, &#39;string&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fc1b1ce-d768-4195-82ad-a5cd2baf4b4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\n|order_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+--------------------+--------+---------------+\norder_customer_id|          order_date|order_id|   order_status|\n+-----------------+--------------------+--------+---------------+\n            11599|2013-07-25 00:00:...|       1|         CLOSED|\n              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n             8827|2013-07-25 00:00:...|       4|         CLOSED|\n            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n             1837|2013-07-25 00:00:...|      12|         CLOSED|\n             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n             1205|2013-07-25 00:00:...|      18|         CLOSED|\n             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n+-----------------+--------------------+--------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6384dc-007f-448e-9233-8093b37344fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"12 Reading JSON Files into Spark Data Frame","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195748291}},"nbformat":4,"nbformat_minor":0}