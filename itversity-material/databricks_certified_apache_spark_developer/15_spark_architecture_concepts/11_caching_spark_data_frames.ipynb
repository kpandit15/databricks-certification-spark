{"cells":[{"cell_type":"markdown","source":["* `persist`\n* `cache` => **persist** with **MEMORY_AND_DISK**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cca3c3e5-9d6d-415e-99d0-af4bacfb953a"}}},{"cell_type":"code","source":["df = spark.read.json('/public/retail_db_json/orders')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"358be46f-5d44-47b3-a7f8-955079fafbce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(df.persist)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56d4deee-5578-4803-8808-038598dac763"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method persist in module pyspark.sql.dataframe:\n\npersist(storageLevel=StorageLevel(True, True, False, True, 1)) method of pyspark.sql.dataframe.DataFrame instance\n    Sets the storage level to persist the contents of the :class:`DataFrame` across\n    operations after the first time it is computed. This can only be used to assign\n    a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n    If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n    \n    .. versionadded:: 1.3.0\n    \n    Notes\n    -----\n    The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method persist in module pyspark.sql.dataframe:\n\npersist(storageLevel=StorageLevel(True, True, False, True, 1)) method of pyspark.sql.dataframe.DataFrame instance\n    Sets the storage level to persist the contents of the :class:`DataFrame` across\n    operations after the first time it is computed. This can only be used to assign\n    a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n    If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n    \n    .. versionadded:: 1.3.0\n    \n    Notes\n    -----\n    The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(df.cache)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d37b969-23d7-48ff-9f59-abfefcd934a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on method cache in module pyspark.sql.dataframe:\n\ncache() method of pyspark.sql.dataframe.DataFrame instance\n    Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n    \n    .. versionadded:: 1.3.0\n    \n    Notes\n    -----\n    The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method cache in module pyspark.sql.dataframe:\n\ncache() method of pyspark.sql.dataframe.DataFrame instance\n    Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n    \n    .. versionadded:: 1.3.0\n    \n    Notes\n    -----\n    The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark import StorageLevel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69fa2fb2-a082-45a0-8e0b-47ee8017f556"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["help(StorageLevel)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b038d591-7ef4-4dae-9618-5fe7f8ee3cf2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Help on class StorageLevel in module pyspark.storagelevel:\n\nclass StorageLevel(builtins.object)\n |  StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)\n |  \n |  Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n |  whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n |  in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n |  nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n |  Since the data is always serialized on the Python side, all the constants use the serialized\n |  formats.\n |  \n |  Methods defined here:\n |  \n |  __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  __str__(self)\n |      Return str(self).\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  DISK_ONLY = StorageLevel(True, False, False, False, 1)\n |  \n |  DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n |  \n |  DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)\n |  \n |  MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n |  \n |  MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n |  \n |  MEMORY_AND_DISK_DESER = StorageLevel(True, True, False, True, 1)\n |  \n |  MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n |  \n |  MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n |  \n |  OFF_HEAP = StorageLevel(True, True, True, False, 1)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on class StorageLevel in module pyspark.storagelevel:\n\nclass StorageLevel(builtins.object)\n  StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)\n  \n  Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n  whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n  in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n  nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n  Since the data is always serialized on the Python side, all the constants use the serialized\n  formats.\n  \n  Methods defined here:\n  \n  __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1)\n      Initialize self.  See help(type(self)) for accurate signature.\n  \n  __repr__(self)\n      Return repr(self).\n  \n  __str__(self)\n      Return str(self).\n  \n  ----------------------------------------------------------------------\n  Data descriptors defined here:\n  \n  __dict__\n      dictionary for instance variables (if defined)\n  \n  __weakref__\n      list of weak references to the object (if defined)\n  \n  ----------------------------------------------------------------------\n  Data and other attributes defined here:\n  \n  DISK_ONLY = StorageLevel(True, False, False, False, 1)\n  \n  DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n  \n  DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)\n  \n  MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n  \n  MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n  \n  MEMORY_AND_DISK_DESER = StorageLevel(True, True, False, True, 1)\n  \n  MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n  \n  MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n  \n  OFF_HEAP = StorageLevel(True, True, True, False, 1)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"398d9ff6-8956-4275-9cc7-b0d97e6e4f20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"11 Caching Spark Data Frames","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":456917195750209}},"nbformat":4,"nbformat_minor":0}