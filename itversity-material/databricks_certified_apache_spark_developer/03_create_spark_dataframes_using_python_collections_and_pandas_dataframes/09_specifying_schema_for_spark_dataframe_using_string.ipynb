{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14b6a601-2c96-46ed-a857-cb5a9a477a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "users = [(1,\n",
    "  'Corrie',\n",
    "  'Van den Oord',\n",
    "  'cvandenoord0@etsy.com',\n",
    "  True,\n",
    "  1000.55,\n",
    "  datetime.date(2021, 1, 15),\n",
    "  datetime.datetime(2021, 2, 10, 1, 15)),\n",
    " (2,\n",
    "  'Nikolaus',\n",
    "  'Brewitt',\n",
    "  'nbrewitt1@dailymail.co.uk',\n",
    "  True,\n",
    "  900.0,\n",
    "  datetime.date(2021, 2, 14),\n",
    "  datetime.datetime(2021, 2, 18, 3, 33)),\n",
    " (3,\n",
    "  'Orelie',\n",
    "  'Penney',\n",
    "  'openney2@vistaprint.com',\n",
    "  True,\n",
    "  850.55,\n",
    "  datetime.date(2021, 1, 21),\n",
    "  datetime.datetime(2021, 3, 15, 15, 16, 55)),\n",
    " (4,\n",
    "  'Ashby',\n",
    "  'Maddocks',\n",
    "  'amaddocks3@home.pl',\n",
    "  False,\n",
    "  None,\n",
    "  None,\n",
    "  datetime.datetime(2021, 4, 10, 17, 45, 30)),\n",
    " (5,\n",
    "  'Kurt',\n",
    "  'Rome',\n",
    "  'krome4@shutterfly.com',\n",
    "  False,\n",
    "  None,\n",
    "  None,\n",
    "  datetime.datetime(2021, 4, 2, 0, 55, 18))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "423e274c-0aa2-4102-a3c1-c87dc16002c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_schema = '''\n",
    "    id INT,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    is_customer BOOLEAN,\n",
    "    amount_paid FLOAT,\n",
    "    customer_from DATE,\n",
    "    last_updated_ts TIMESTAMP\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6cd63542-4a8e-4d50-9b69-be4b772932bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9551596f-89ab-4c2a-b5a7-dc5653074a49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>first_name</th><th>last_name</th><th>email</th><th>is_customer</th><th>amount_paid</th><th>customer_from</th><th>last_updated_ts</th></tr>\n",
       "<tr><td>1</td><td>Corrie</td><td>Van den Oord</td><td>cvandenoord0@etsy...</td><td>true</td><td>1000.55</td><td>2021-01-15</td><td>2021-02-10 01:15:00</td></tr>\n",
       "<tr><td>2</td><td>Nikolaus</td><td>Brewitt</td><td>nbrewitt1@dailyma...</td><td>true</td><td>900.0</td><td>2021-02-14</td><td>2021-02-18 03:33:00</td></tr>\n",
       "<tr><td>3</td><td>Orelie</td><td>Penney</td><td>openney2@vistapri...</td><td>true</td><td>850.55</td><td>2021-01-21</td><td>2021-03-15 15:16:55</td></tr>\n",
       "<tr><td>4</td><td>Ashby</td><td>Maddocks</td><td>amaddocks3@home.pl</td><td>false</td><td>null</td><td>null</td><td>2021-04-10 17:45:30</td></tr>\n",
       "<tr><td>5</td><td>Kurt</td><td>Rome</td><td>krome4@shutterfly...</td><td>false</td><td>null</td><td>null</td><td>2021-04-02 00:55:18</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
       "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
       "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
       "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
       "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
       "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
       "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n",
       "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n",
       "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=users_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9c36b6d-82fb-4420-bc4f-f543623ca8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=users_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df70e88c-28ed-4676-9d4c-d0422831acd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09 Specifying Schema for Spark Dataframe using String",
   "notebookOrigID": 456917195750000,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
